{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607e5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, torch\n",
    "import numpy as np\n",
    "from models.imagetransformer import ImageTransformer\n",
    "from dataset_utils.training_dataset_creation import TrainDataset\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc91965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3DDFA Packages\n",
    "from models.DDFA import *\n",
    "import yaml\n",
    "from FaceBoxes import FaceBoxes\n",
    "from TDDFA import TDDFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b748cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5cdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "train_dir_real = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/real/Images/RealTrainSet/real/'\n",
    "train_dir_fake = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeTrainSet/FaceSwap/'\n",
    "train_dir_fake_2 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeTrainSet/Face2Face/'\n",
    "train_dir_fake_3 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeTrainSet/Deepfakes/'\n",
    "train_dir_fake_4 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeTrainSet/NeuralTextures/'\n",
    "\n",
    "\n",
    "valid_dir_real = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/real/Images/RealValidationSet/real/'\n",
    "valid_dir_fake = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeValidationSet/FaceSwap/'\n",
    "valid_dir_fake_2 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeValidationSet/Face2Face/'\n",
    "valid_dir_fake_3 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeValidationSet/Deepfakes/'\n",
    "valid_dir_fake_4 = '/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Datasets/DeepfakeDetection/FaceForensics/fake/Images/FakeValidationSet/NeuralTextures/'\n",
    "\n",
    "paths.append(train_dir_real)\n",
    "paths.append(train_dir_fake)\n",
    "paths.append(train_dir_fake_2)\n",
    "paths.append(train_dir_fake_3)\n",
    "paths.append(train_dir_fake_4)\n",
    "paths.append(valid_dir_real)\n",
    "paths.append(valid_dir_fake)\n",
    "paths.append(valid_dir_fake_2)\n",
    "paths.append(valid_dir_fake_3)\n",
    "paths.append(valid_dir_fake_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c141d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Real: 106543\n",
      "Train Data Fake: 36155\n",
      "Train Data Fake 2: 36156\n",
      "Train Data Fake 3: 36121\n",
      "Train Data Fake 4: 36131\n",
      "Labels: ['Face2Face', 'Deepfakes', 'FaceSwap', 'FaceSwap', 'Face2Face', 'real', 'NeuralTextures', 'FaceSwap', 'real', 'real']\n",
      "200000 33334\n",
      "40000 6667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "train_loader, valid_loader = TrainDataset.get_image_batches(paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e177bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Resized positional embeddings from torch.Size([1, 577, 768]) to torch.Size([1, 325, 768])\n",
      "Missing keys when loading pretrained weights: ['patch_embedding.weight', 'patch_embedding.bias', 'sequence_embedding.seq_embedding', 'linear_1.weight', 'linear_1.bias', 'proj.weight', 'proj.bias', 'fc.weight', 'fc.bias']\n",
      "Unexpected keys when loading pretrained weights: []\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "model = ImageTransformer('B_16_imagenet1k', pretrained=True, image_size = 300, num_classes = 2,\n",
    "                        seq_embed=True, hybrid=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd63f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "lr = 3e-3\n",
    "# gamma = 0.7\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f189ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 162, 768])\n",
      "torch.Size([1, 162, 768])\n",
      "torch.Size([1, 162, 768])\n",
      "torch.Size([1, 162, 768])\n",
      "torch.Size([1, 162, 768])\n",
      "torch.Size([1, 162, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12956/1329258666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#         print(\"HERE\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python_3_gpss\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python_3_gpss\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NO SegEmbed f2f only eval on with_new_train_valid_strategy\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for data, label in (train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "#         print(\"HERE\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in (valid_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "#             print(data.shape)aa\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "    \n",
    "#     if epoch == 1:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_2epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 2:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_3epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 3:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_4epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 4:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_5epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 5:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_6epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 6:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_7epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "#     elif epoch == 7:\n",
    "#         PATH = (\"/Users/Sohail/Desktop/Research/PhD/Year1/NewDeepfakeDetector/Code/new_ensemble_vit_8epochs_fs_df_f2f_nt.pth\")\n",
    "#         torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "        \n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in train_loader:\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bde0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
